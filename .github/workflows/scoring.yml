name: GNN Challenge Auto-Grader

on:
  pull_request:
    paths:
      # Updated to catch any CSV inside the submissions folder
      - 'submissions/**/*.csv'

permissions:
  contents: write
  pull-requests: write

jobs:
  grade:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Important: Fetches history so 'git diff' works

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install Dependencies
        run: pip install pandas scikit-learn

      # - name: Restore Secret Answer Key
      #   # Decodes the hidden labels from GitHub Secrets
      #   env:
      #     TEST_LABELS_BASE64: ${{ secrets.TEST_LABELS_BASE64 }}
      #   run: |
      #     echo "$TEST_LABELS_BASE64" | base64 -d > data/test_labels_hidden.csv

        - name: Restore Secret Answer Key
        # Decodes the hidden labels from GitHub Secrets
        env:
          TEST_LABELS_BASE64: ${{ secrets.TEST_LABELS_BASE64 }}
        run: |
          # Now decodes AND unzips the file
          echo "$TEST_LABELS_BASE64" | base64 -d | gzip -d > data/test_labels_hidden.csv

      - name: Identify Submission File
        id: find_file
        run: |
          # Find the CSV file that changed in this PR (ignoring sample_submission)
          # We search specifically in the 'submissions/' directory
          FILE=$(git diff --name-only origin/main HEAD | grep 'submissions/.*.csv' | grep -v 'sample_submission.csv' | head -n 1)
          
          if [ -z "$FILE" ]; then
            echo "::error::No valid submission file found in this PR."
            exit 1
          fi
          
          echo "Found file: $FILE"
          # Save the path to the Environment Variable so all other steps can use it
          echo "FILE_PATH=$FILE" >> $GITHUB_ENV

      - name: Validate Submission (Rules Check)
        run: |
          # 1. Find the metadata.json file (it must be in the same folder as the CSV)
          META_FILE=$(dirname "$FILE_PATH")/metadata.json
          
          if [ ! -f "$META_FILE" ]; then
            echo "::error::metadata.json is MISSING! Please include it in your folder."
            exit 1
          fi

          # 2. Run the Python Validation Script
          # Arguments: [Predictions] [Test Nodes] [Metadata] [Leaderboard]
          python competition/validate_submission.py "$FILE_PATH" "data/public/test_nodes.csv" "$META_FILE" "leaderboard/leaderboard.csv"

      - name: Run Scoring Script
        id: score
        run: |
          # Run the evaluation script we created earlier
          # Arguments: [Predictions] [Hidden True Labels]
          OUTPUT=$(python competition/evaluate.py "$FILE_PATH" "data/test_labels_hidden.csv")
          
          echo "$OUTPUT"
          
          # Extract score (Looking for "SCORE=0.xxxx")
          SCORE=$(echo "$OUTPUT" | grep -oP 'SCORE=\K[0-9.]+')
          echo "extracted_score=$SCORE" >> $GITHUB_OUTPUT

      - name: Comment on PR
        uses: tholman/github-action-comment-pull-request@v2
        with:
          message: |
            ### ðŸ¤– Automated Grading Report
            | Metric | Score | Status |
            | :--- | :--- | :--- |
            | **Macro F1** | **${{ steps.score.outputs.extracted_score }}** | âœ… Valid |
            
            *The leaderboard will be updated if this PR is merged.*
